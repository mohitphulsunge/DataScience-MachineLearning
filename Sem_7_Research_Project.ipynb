{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohitphulsunge/DataScience-MachineLearning/blob/main/Sem_7_Research_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wHl8ib1S-_QB"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "\n",
        "# classes name which are given in CIFAR-10 dataset(10 types of images in this dataset which are given below)\n",
        "class_name = [\"aeroplane\",\"automobile\",\"bird\",\"cat\",\"deer\",\"dog\",\"frog\",\"horse\",\"ship\",\"truck\"]\n",
        "\n",
        "#this function gives you predicted class name by CNN with highest probabilistic class\n",
        "def classify_name(predicts):\n",
        "    max =predicts[0,0]\n",
        "    temp =0\n",
        "    for i in range(len(predicts[0])):\n",
        "        #check higher probable class \n",
        "        if predicts[0,i]>max:\n",
        "                max = predicts[0,i]\n",
        "                temp = i\n",
        "    # print higher probale class name\n",
        "    print(class_name[temp])\n",
        "    \n",
        "# this function loads dataset as numpy array and divides into training set, validation set and test set with standardization\n",
        "def load_dataset(dirpath='<dataset path of CIFAR10 for python>'):# give path as example \"/home/username/folder name\"\n",
        "    X, y = [], []\n",
        "    # take data from the data batch\n",
        "    for path in glob.glob('%s/data_batch_*' % dirpath):\n",
        "        with open(path, 'rb') as f:\n",
        "            batch = pickle.load(f)#,encoding='latin1' (if gives error of encoding)\n",
        "        # append all data and labels from the 5 data betch\n",
        "        X.append(batch['data'])\n",
        "        y.append(batch['labels'])\n",
        "    # devide by 255 for making value 0 to 1\n",
        "    X = np.concatenate(X) /np.float32(255)\n",
        "    # making labels as int\n",
        "    y = np.concatenate(y).astype(np.int32)\n",
        "    #seperate in to RGB colors\n",
        "    X = np.dstack((X[:, :1024], X[:, 1024:2048], X[:, 2048:]))\n",
        "    # reshape data into 4D tensor with compatible to CNN model\n",
        "    X = X.reshape((X.shape[0], 32, 32, 3))\n",
        "    # initialize labels for training ,validation and testing \n",
        "    Y_train = np.zeros((40000,10),dtype = np.float32)\n",
        "    Y_valid = np.zeros((10000,10), dtype = np.float32)\n",
        "    y_test = np.zeros((10000,10),dtype = np.int32)\n",
        "    \n",
        "    # divide 40000 as training data and it's labels\n",
        "    X_train = X[-40000:]\n",
        "    y_train = y[-40000:]\n",
        "    #devide 10000 as validation data and it's labelss\n",
        "    X_valid = X[:-40000]\n",
        "    y_valid = y[:-40000]\n",
        "    \n",
        "    # make training labels compatables with CNN model\n",
        "    for i in range(40000):\n",
        "        a = y_train[i]\n",
        "        Y_train[i,a] = 1\n",
        "\n",
        "    # make validation labels compatables with CNN model\n",
        "    for i in range(10000):\n",
        "        a = y_valid[i]\n",
        "        Y_valid[i,a] = 1\n",
        "    \n",
        "    # load test set\n",
        "    path = '%s/test_batch' % dirpath\n",
        "    with open(path, 'rb') as f:\n",
        "        batch = pickle.load(f)#,encoding='latin1'\n",
        "    X_test = batch['data'] /np.float32(255)\n",
        "    X_test = np.dstack((X_test[:, :1024], X_test[:, 1024:2048], X_test[:, 2048:]))\n",
        "    X_test = X_test.reshape((X_test.shape[0], 32, 32, 3))\n",
        "    y_t = np.array(batch['labels'], dtype=np.int32)\n",
        "    # make test labels compatables with CNN model\n",
        "    for i in range(10000):\n",
        "        a = y_t[i]\n",
        "        y_test[i,a] = 1\n",
        "\n",
        "    # normalize to zero mean and unity variance\n",
        "    offset = np.mean(X_train, 0)\n",
        "    scale = np.std(X_train, 0).clip(min=1)\n",
        "    X_train = (X_train - offset) / scale\n",
        "    X_valid = (X_valid - offset) / scale\n",
        "    X_test = (X_test - offset) / scale\n",
        "    return X_train, Y_train, X_valid, Y_valid, X_test, y_test\n",
        "\n",
        "# this function is used as divide input data and labels in mini batch(batchsize) and also used shuffle to give some randomness to CNN \n",
        "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
        "    assert len(inputs) == len(targets)\n",
        "    # shuffle is used in train the data\n",
        "    if shuffle:\n",
        "        indices = np.arange(len(inputs))\n",
        "        np.random.shuffle(indices)\n",
        "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
        "        if shuffle:\n",
        "            excerpt = indices[start_idx:start_idx + batchsize]\n",
        "        else:\n",
        "            excerpt = slice(start_idx, start_idx + batchsize)\n",
        "        yield inputs[excerpt], targets[excerpt]\n",
        "\n",
        "# Convolution neural network model\n",
        "# {conv(with relu) -> max_pool -> conv(with relu) -> max_pool -> conv(with relu) -> max_pool -> dense layer -> [output(train), softmax(main predictionss)]} \n",
        "def build_model(input_val,w,b):\n",
        "\n",
        "    conv1 = tf.nn.conv2d(input_val,w['w1'],strides = [1,1,1,1], padding = 'SAME')\n",
        "    conv1 = tf.nn.bias_add(conv1,b['b1'])\n",
        "    conv1 = tf.nn.relu(conv1)\n",
        "    pool1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "    conv2 = tf.nn.conv2d(pool1,w['w2'],strides = [1,1,1,1], padding = 'SAME')\n",
        "    conv2 = tf.nn.bias_add(conv2,b['b2'])\n",
        "    conv2 = tf.nn.relu(conv2)\n",
        "    pool2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "    conv3 = tf.nn.conv2d(pool2,w['w3'],strides = [1,1,1,1], padding = 'SAME')\n",
        "    conv3 = tf.nn.bias_add(conv3,b['b3'])\n",
        "    conv3 = tf.nn.relu(conv3)  \n",
        "    pool3 = tf.nn.max_pool(conv3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
        "\n",
        "    shape = pool3.get_shape().as_list()\n",
        "    dense = tf.reshape(pool3,[-1,shape[1]*shape[2]*shape[3]])\n",
        "    dense1 = tf.nn.relu(tf.nn.bias_add(tf.matmul(dense,w['w4']),b['b4']))\n",
        "    \n",
        "    # used for training the CNN model\n",
        "    out = tf.nn.bias_add(tf.matmul(dense1,w['w5']),b['b5'])\n",
        "\n",
        "    # used after training the CNN\n",
        "    softmax = tf.nn.softmax(out)\n",
        "    \n",
        "    return out,softmax\n",
        "\n",
        "# main function where network train and predict the output on random image\n",
        "def main_function(num_epochs=100):\n",
        "    \n",
        "    # initialize input data shape and datatype for data and labels\n",
        "    x = tf.placeholder(tf.float32,[None,32,32,3])\n",
        "    y = tf.placeholder(tf.int32,[None,10])\n",
        "    \n",
        "    # initialize weights for every different layers\n",
        "    weights = {\n",
        "        'w1': tf.Variable(tf.random_normal([5,5,3,120],stddev = 0.1)),\n",
        "        'w2': tf.Variable(tf.random_normal([5,5,120,60],stddev = 0.1)),\n",
        "        'w3': tf.Variable(tf.random_normal([4,4,60,30],stddev = 0.1)),\n",
        "        'w4': tf.Variable(tf.random_normal([4*4*30,30],stddev = 0.1)),\n",
        "        'w5': tf.Variable(tf.random_normal([30,10],stddev = 0.1))\n",
        "    }\n",
        "\n",
        "    # initialize biases for every different layers\n",
        "    biases = {\n",
        "        'b1': tf.Variable(tf.random_normal([120],stddev = 0.1)),\n",
        "        'b2': tf.Variable(tf.random_normal([60],stddev = 0.1)),\n",
        "        'b3': tf.Variable(tf.random_normal([30],stddev = 0.1)),\n",
        "        'b4': tf.Variable(tf.random_normal([30],stddev = 0.1)),\n",
        "        'b5': tf.Variable(tf.random_normal([10],stddev = 0.1))\n",
        "    }\n",
        "\n",
        "    # call model \n",
        "    predict,out_predict = build_model(x,weights,biases)\n",
        "    # whole back propagetion process\n",
        "    error = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = predict,labels = y))\n",
        "    optm = tf.train.AdamOptimizer(learning_rate = 0.01).minimize(error)\n",
        "    corr = tf.equal(tf.argmax(predict,1),tf.argmax(y,1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(corr,tf.float32))\n",
        "    # initialize saver for saving weight and bias values\n",
        "    saver = tf.train.Saver()\n",
        "\n",
        "    init = tf.global_variables_initializer()\n",
        "    if not os.path.exists('<path for weight files>/model.ckpt.meta'): \n",
        "        # initialize tensorflow session\n",
        "        sess = tf.Session()\n",
        "        sess.run(init)\n",
        "        # load dataset \n",
        "        print(\"loading dataset...\")\n",
        "        X_train,y_train,X_val, y_val,X_test,y_test = load_dataset()\n",
        "        # training will start\n",
        "        print(\"Starting training...\")\n",
        "        for epoch in range(num_epochs):\n",
        "            train_err = 0\n",
        "            train_acc = 0\n",
        "            train_batches = 0\n",
        "            start_time = time.time()\n",
        "            # devide data into mini batch\n",
        "            for batch in iterate_minibatches(X_train, y_train, 500, shuffle=True):\n",
        "                inputs, targets = batch\n",
        "                # this is update weights\n",
        "                sess.run([optm],feed_dict = {x: inputs,y: targets})\n",
        "                # cost function\n",
        "                err,acc= sess.run([error,accuracy],feed_dict = {x: inputs,y: targets})\n",
        "                train_err += err\n",
        "                train_acc += acc\n",
        "                train_batches += 1\n",
        "                \n",
        "            val_err = 0\n",
        "            val_acc = 0\n",
        "            val_batches = 0\n",
        "            # divide validation data into mini batch without shuffle\n",
        "            for batch in iterate_minibatches(X_val, y_val, 500, shuffle=False):\n",
        "                inputs, targets = batch\n",
        "                # this is update weights\n",
        "                sess.run([optm],feed_dict = {x: inputs,y: targets})\n",
        "                # cost function\n",
        "                err, acc = sess.run([error,accuracy],feed_dict = {x: inputs,y: targets})\n",
        "                val_err += err\n",
        "                val_acc += acc\n",
        "                val_batches += 1\n",
        "            # print present epoch with total number of epoch\n",
        "            # print training and validation loss with accuracy\n",
        "            print(\"Epoch {} of {} took {:.3f}s\".format(\n",
        "                    epoch + 1, num_epochs, time.time() - start_time))\n",
        "            print(\"  training loss:\\t\\t{:.6f}\".format(train_err / train_batches))\n",
        "            print(\"  validation loss:\\t\\t{:.6f}\".format(val_err / val_batches))\n",
        "            print(\"  validation accuracy:\\t\\t{:.2f} %\".format(\n",
        "                    val_acc / val_batches * 100))\n",
        "        \n",
        "        # testing using test dataset as per above    \n",
        "        test_err = 0\n",
        "        test_acc = 0\n",
        "        test_batches = 0\n",
        "        for batch in iterate_minibatches(X_test, y_test, 500, shuffle=False):\n",
        "            inputs, targets = batch\n",
        "            err, acc = sess.run([error,accuracy],feed_dict = {x: inputs,y: targets})# apply tensor function\n",
        "            test_err += err\n",
        "            test_acc += acc\n",
        "            test_batches += 1\n",
        "        print(\"Final results:\")\n",
        "        print(\"  test loss:\\t\\t\\t{:.6f}\".format(test_err / test_batches))\n",
        "        print(\"  test accuracy:\\t\\t{:.2f} %\".format(\n",
        "                test_acc / test_batches * 100))\n",
        "        # save weights values in ckpt file in given folder path\n",
        "        save_path = saver.save(sess,\"<path for saving the weights>/model.ckpt\")\n",
        "\n",
        "    #if you have pre-trained data this else portion will be used\n",
        "    else:\n",
        "        sess = tf.Session()\n",
        "        sess.run(init)\n",
        "        #restore weights value for this CNN  \n",
        "        saver.restore(sess,\"<path of weight file>/model.ckpt\")\n",
        "    \n",
        "    # testing random image from the anywhere\n",
        "    img = cv2.imread('<path for outer test image>/sample.jpg')\n",
        "    new_img = cv2.resize(img,dsize = (32,32),interpolation = cv2.INTER_CUBIC)\n",
        "    new_img = np.asarray(new_img, dtype='float32') / 256.\n",
        "    img_ = new_img.reshape((-1, 32, 32, 3))\n",
        "    # output prediction for above image it gives 10 numeric numbers with it's class probability\n",
        "    prediction = sess.run(out_predict,feed_dict={x: img_})\n",
        "    # print predicted sclass\n",
        "    classify_name(prediction)\n",
        "    sess.close()\n",
        "\n",
        "# call main function to run whole code\n",
        "if __name__ == '__main__':\n",
        "    main_function()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}